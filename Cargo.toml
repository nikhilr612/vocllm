[package]
name = "vocllm"
version = "0.1.0"
edition = "2021"
description = "An elementary chat interface for local quantized LLMs, with optional native tts. Currently, only Mistral and Llama models are supported."

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
candle-transformers = { version = "0.4.1", features = ["cuda"] }
candle-core = { version = "0.4.1", features = ["cuda"] }
clap = { version = "4.5.4", features = ["derive"] }
pretty_env_logger = "0.5.0"
log = { version = "0.4.21", features = ["release_max_level_debug"] }
tokenizers = "0.19.1"
serde_json = "1.0.116"

[profile.release]
opt-level = 3

[profile.native]
inherits = "release"
# The most unhinged build profile.
opt-level = 3
strip = "symbols"
lto = "fat"
overflow-checks = false
panic = "abort"
debug-assertions = false
